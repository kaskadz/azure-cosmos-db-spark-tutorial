{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADZD\n",
    "# Cosmos DB Real-time Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - dataframes\n",
    "Read dataframe from CosmosDB container - __invoices__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "    .format(\"cosmos.olap\")\\\n",
    "    .option(\"spark.synapse.linkedService\", \"CosmosDb1\")\\\n",
    "    .option(\"spark.cosmos.container\", \"invoices\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- _rid: string (nullable = true)\n",
       " |-- _ts: long (nullable = true)\n",
       " |-- InvoiceDate: string (nullable = true)\n",
       " |-- Country: string (nullable = true)\n",
       " |-- CustomerId: long (nullable = true)\n",
       " |-- Items: array (nullable = true)\n",
       " |    |-- element: struct (containsNull = true)\n",
       " |    |    |-- StockCode: string (nullable = true)\n",
       " |    |    |-- Description: string (nullable = true)\n",
       " |    |    |-- Quantity: long (nullable = true)\n",
       " |    |    |-- UnitPrice: double (nullable = true)\n",
       " |-- id: string (nullable = true)\n",
       " |-- _etag: string (nullable = true)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write query to create new dataframe consisting of two columns -> __id__ ( customerID ) and __ClientMoneySpent__ ( money which each client spent according to invoices). Need to write some aggregations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df_clients = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- id: string (nullable = true)\n",
       " |-- ClientMoneySpent: double (nullable = true)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clients.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----+-------------------+\n",
       "|   id|   ClientMoneySpent|\n",
       "+-----+-------------------+\n",
       "|17850|  5391.210000000009|\n",
       "|13047|  366.6300000000001|\n",
       "|12583|             855.86|\n",
       "|13748|              204.0|\n",
       "|15100|              350.4|\n",
       "|15291|              328.8|\n",
       "|14688|             444.98|\n",
       "|17809|             1251.5|\n",
       "|15311|            1095.97|\n",
       "|14527| 236.15999999999994|\n",
       "|16098| 430.59999999999997|\n",
       "|18074|              489.6|\n",
       "|17420|             130.85|\n",
       "|16029|            4271.52|\n",
       "|16250|             226.14|\n",
       "|12431|             358.25|\n",
       "|17511|            1825.74|\n",
       "|17548|-141.48000000000002|\n",
       "|13705| 318.14000000000004|\n",
       "|13747|               79.6|\n",
       "+-----+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Spark DataFrame into a Cosmos DB container\n",
    "# To select a preferred list of regions in a multi-region Cosmos DB account, add .option(\"spark.cosmos.preferredRegions\", \"<Region1>,<Region2>\")\n",
    "\n",
    "df_clients.write\\\n",
    "    .format(\"cosmos.oltp\")\\\n",
    "    .option(\"spark.synapse.linkedService\", \"CosmosDb1\")\\\n",
    "    .option(\"spark.cosmos.container\", \"clients\")\\\n",
    "    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
    "    .mode('append')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - streams\n",
    "Read spark stream from invoices and write aggregated stream to __Countries__ container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a streaming Spark DataFrame from a Cosmos DB container\n",
    "# To select a preferred list of regions in a multi-region Cosmos DB account, add .option(\"spark.cosmos.preferredRegions\", \"<Region1>,<Region2>\")\n",
    "\n",
    "dfStream = spark.readStream\\\n",
    "    .format(\"cosmos.oltp\")\\\n",
    "    .option(\"spark.synapse.linkedService\", \"CosmosDb1\")\\\n",
    "    .option(\"spark.cosmos.container\", \"invoices\")\\\n",
    "    .option(\"spark.cosmos.changeFeed.readEnabled\", \"true\")\\\n",
    "    .option(\"spark.cosmos.changeFeed.startFromTheBeginning\", \"true\")\\\n",
    "    .option(\"spark.cosmos.changeFeed.checkpointLocation\", \"/localReadCheckpointFolder\")\\\n",
    "    .option(\"spark.cosmos.changeFeed.queryName\", \"streamQuery\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- Country: string (nullable = true)\n",
       " |-- CustomerId: integer (nullable = true)\n",
       " |-- InvoiceDate: string (nullable = true)\n",
       " |-- Items: array (nullable = true)\n",
       " |    |-- element: struct (containsNull = false)\n",
       " |    |    |-- UnitPrice: double (nullable = true)\n",
       " |    |    |-- Description: string (nullable = true)\n",
       " |    |    |-- Quantity: integer (nullable = true)\n",
       " |    |    |-- StockCode: string (nullable = true)\n",
       " |-- _attachments: string (nullable = true)\n",
       " |-- _etag: string (nullable = true)\n",
       " |-- _rid: string (nullable = true)\n",
       " |-- _self: string (nullable = true)\n",
       " |-- _ts: integer (nullable = true)\n",
       " |-- id: string (nullable = true)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfStream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write aggregations, stream should return __id__ (Country) and __Invoices__ ( How many invoices are from current country)<br>\n",
    "Remember about creating additional column which holds timestamp (created from 'InvoiceDate') and setting watermark on this column, e.g. for 10 minutes.\n",
    "HINT: _to_timestamp(?)_ , _with_watermark(?,?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStream_countries = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- id: string (nullable = true)\n",
       " |-- Invoices: long (nullable = false)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfStream_countries.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamQuery = dfStream_countries\\\n",
    "    .writeStream\\\n",
    "    .format(\"cosmos.oltp\")\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .option(\"checkpointLocation\", \"/localWriteCheckpointFolder\")\\\n",
    "    .option(\"spark.synapse.linkedService\", \"CosmosDb1\")\\\n",
    "    .option(\"spark.cosmos.container\", \"countries\")\\\n",
    "    .option(\"spark.cosmos.connection.mode\", \"gateway\")\\\n",
    "    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "== Physical Plan ==\n",
       "*(5) HashAggregate(keys=[Country#332], functions=[count(1)])\n",
       "+- StateStoreSave [Country#332], state info [ checkpoint = abfss://fdlsg2@dlsg2acc.dfs.core.windows.net/localWriteCheckpointFolder/state, runId = abca3a6e-7519-4f88-b787-7d7a39517c04, opId = 0, ver = 0, numPartitions = 200], Update, 0, 2\n",
       "   +- *(4) HashAggregate(keys=[Country#332], functions=[merge_count(1)])\n",
       "      +- StateStoreRestore [Country#332], state info [ checkpoint = abfss://fdlsg2@dlsg2acc.dfs.core.windows.net/localWriteCheckpointFolder/state, runId = abca3a6e-7519-4f88-b787-7d7a39517c04, opId = 0, ver = 0, numPartitions = 200], 2\n",
       "         +- *(3) HashAggregate(keys=[Country#332], functions=[merge_count(1)])\n",
       "            +- Exchange hashpartitioning(Country#332, 200), [id=#135]\n",
       "               +- *(2) HashAggregate(keys=[Country#332], functions=[partial_count(1)])\n",
       "                  +- *(2) Project [Country#332]\n",
       "                     +- EventTimeWatermark ts#257: timestamp, interval 10 minutes\n",
       "                        +- *(1) Project [Country#332, cast(InvoiceDate#334 as timestamp) AS ts#257]\n",
       "                           +- Scan ExistingRDD[Country#332,CustomerId#333,InvoiceDate#334,Items#335,_attachments#336,_etag#337,_rid#338,_self#339,_ts#340,id#341]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamQuery.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream actual status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamQuery.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "saveOutput": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
